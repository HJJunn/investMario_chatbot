{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. 테스트 데이터 전처리"
      ],
      "metadata": {
        "id": "aDzZ2Lmd9-ny"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tz4bEtknynwd"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "import pandas as pd\n",
        "from typing import List, Dict\n",
        "from datasets import load_dataset, Dataset\n",
        "from vllm import LLM, Samplingparams"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 허깅페이스 허브에서 데이터셋 로드\n",
        "dataset = load_dataset(\"HJUNN/crypto_function_calling_datasets\", split = \"train\")"
      ],
      "metadata": {
        "id": "PG987fzOzQKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테스트 비율 설정\n",
        "test_ratio = 0.2\n",
        "\n",
        "total_len = len(dataset)\n",
        "test_size = int(total_len * test_ratio)\n",
        "\n",
        "test_indices = list(range(test_size))\n",
        "train_indices = list(range(test_size, total_len))"
      ],
      "metadata": {
        "id": "yq4WjNNozokI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#OpenAI 포맷으로 변환 함수\n",
        "def format_conversation(sample):\n",
        "    return {\n",
        "        \"messages\" : [\n",
        "            {\"role\" : \"system\", \"content\" : sample[\"system_prompt\"]},\n",
        "            *sample[\"messages\"]\n",
        "        ]\n",
        "    }\n",
        "# 분할 및 변환\n",
        "train_dataset = [format_conversation(dataset[i]) for i in train_indices]\n",
        "test_dataset = [format_conversation(dataset[i]) for i in test_indices]\n",
        "\n",
        "# Dataset 객체로 변환\n",
        "train_dataset = Dataset.from_list(train_dataset)\n",
        "test_dataset = Dataset.from_list(test_dataset)"
      ],
      "metadata": {
        "id": "cW7bAVqmz38Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_chatml(data):\n",
        "    messages = data.get(\"messages\") if isinstance(data, dict) and \"messages\" in data else data\n",
        "\n",
        "    parts = []\n",
        "    for msg in messages:\n",
        "        role = msg[\"role\"]\n",
        "        content = msg[\"content\"]\n",
        "        parts.append(f\"<|im_start|>{role}\\n{content}<|im_end|>\")\n",
        "    return \"\\n\".join(parts)"
      ],
      "metadata": {
        "id": "FtpTIJ_yz3uM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_examples(chatml:str) -> List[Dict[str, str]]:\n",
        "    examples : List[Dict[str, str]] = []\n",
        "    pattern = re.compile(r'<\\|im_start\\|>assistant(.*?)(?=<\\|im_end\\|>)', re.DOTALL)\n",
        "\n",
        "    for match in pattern.finditer(chatml):\n",
        "        start_idx = match.start()\n",
        "        input_text = chatml[:start_idx].strip() + '\\n<|im_start|>assistant'\n",
        "        label_text = match.group(1).strip()\n",
        "        examples.append({\n",
        "            \"input\" : input_text,\n",
        "            \"label\" : label_text\n",
        "        })\n",
        "\n",
        "    return examples"
      ],
      "metadata": {
        "id": "a4XFqC9018Lr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_lst = []\n",
        "label_lst = []\n",
        "\n",
        "for item in test_dataset:\n",
        "    chatml = to_chatml(item)\n",
        "    examples  = extract_examples(chatml)\n",
        "\n",
        "    for ex in examples:\n",
        "        prompt_lst.append(ex[\"input\"])\n",
        "        label_lst.append(ex[\"label\"])"
      ],
      "metadata": {
        "id": "D73entda9rfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(label_ls[0])"
      ],
      "metadata": {
        "id": "-ZJyYi5w96wK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. 모델 호출"
      ],
      "metadata": {
        "id": "5I-Bu8fF99nf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampling_params = SamplingParams(\n",
        "    temperature = 0,\n",
        "    max_tokens = 2048,\n",
        "    stop = [\"<|im_end|>\"]\n",
        ")"
      ],
      "metadata": {
        "id": "qDiS-O91-BFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = LLM(model = \"Qwen/Qwen2.5-7B-Instruct\")"
      ],
      "metadata": {
        "id": "ndZyB4Tw-Nfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_outputs = llm.generate(prompt_lst, sampling_params)\n",
        "base_model_text_results = [sample.outputs[0].text.strip() for sample in base_mdel_outputs]"
      ],
      "metadata": {
        "id": "ccK-Yr6A-Uvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(base_model_text_results[10])"
      ],
      "metadata": {
        "id": "fAjX6M32-lD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. 평가 결과 저장"
      ],
      "metadata": {
        "id": "3jByaTMq-oAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\n",
        "    \"prompt\" : prompt_lst,\n",
        "    \"label\" : label_lst,\n",
        "    \"output\" : base_model_text_results\n",
        "})\n",
        "\n",
        "df.to_csv(\"base_model_evaluation_results.csv\", index = False, encoding = \"utf-8-sig\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "yYwtsP8T-niT",
        "outputId": "2667982d-e5ed-43a0-e339-b5ec257e53ff"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'pd' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2437289457.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. 평가"
      ],
      "metadata": {
        "id": "SpccgzIs_Ll8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BFCL 확장 평가\n",
        "import re\n",
        "import json\n",
        "\n",
        "def evaluate_function_calls_bfcl(labels, predictions):\n",
        "    \"\"\"\n",
        "    BFCL 스타일 function calling 평가 함수\n",
        "    \"\"\"\n",
        "\n",
        "    results = {\n",
        "        \"tool_selection\": {\"correct\": 0, \"total\": 0},\n",
        "        \"params_selection\": {\"correct\": 0, \"total\": 0},\n",
        "        \"params_value_accuracy\": {\"correct\": 0.0, \"total\": 0},\n",
        "        \"hallucinated_args\": {\"count\": 0},\n",
        "        \"over_call\": {\"count\": 0},\n",
        "        \"under_call\": {\"count\": 0},\n",
        "        \"format_error\": {\"count\": 0}\n",
        "    }\n",
        "\n",
        "    tool_call_pattern = re.compile(r\"<tool_call>(.*?)</tool_call>\", re.DOTALL)\n",
        "    tool_call_count = 0\n",
        "\n",
        "    for label, pred in zip(labels, predictions):\n",
        "\n",
        "        label_match = tool_call_pattern.search(label)\n",
        "        pred_match = tool_call_pattern.search(pred)\n",
        "\n",
        "        # ---------- BFCL: over-call / under-call ----------\n",
        "        if label_match and not pred_match:\n",
        "            results[\"under_call\"][\"count\"] += 1\n",
        "            continue\n",
        "\n",
        "        if not label_match and pred_match:\n",
        "            results[\"over_call\"][\"count\"] += 1\n",
        "            continue\n",
        "\n",
        "        if not label_match:\n",
        "            continue\n",
        "\n",
        "        tool_call_count += 1\n",
        "\n",
        "        # ---------- format robustness ----------\n",
        "        try:\n",
        "            label_json = json.loads(label_match.group(1))\n",
        "            pred_json = json.loads(pred_match.group(1))\n",
        "        except Exception:\n",
        "            results[\"format_error\"][\"count\"] += 1\n",
        "            continue\n",
        "\n",
        "        # ---------- 1. Tool selection ----------\n",
        "        results[\"tool_selection\"][\"total\"] += 1\n",
        "        if label_json.get(\"name\") == pred_json.get(\"name\"):\n",
        "            results[\"tool_selection\"][\"correct\"] += 1\n",
        "\n",
        "        # ---------- 2. Parameter selection ----------\n",
        "        label_params = set(label_json.get(\"arguments\", {}).keys())\n",
        "        pred_params = set(pred_json.get(\"arguments\", {}).keys())\n",
        "\n",
        "        # 정답 파라미터 기준 평가\n",
        "        for p in label_params:\n",
        "            results[\"params_selection\"][\"total\"] += 1\n",
        "            if p in pred_params:\n",
        "                results[\"params_selection\"][\"correct\"] += 1\n",
        "\n",
        "        # 환각 파라미터\n",
        "        hallucinated = pred_params - label_params\n",
        "        results[\"hallucinated_args\"][\"count\"] += len(hallucinated)\n",
        "        results[\"params_selection\"][\"total\"] += len(hallucinated)\n",
        "\n",
        "        # ---------- 3. Parameter value accuracy (partial scoring) ----------\n",
        "        common_params = label_params & pred_params\n",
        "        if common_params:\n",
        "            match_cnt = 0\n",
        "            for k in common_params:\n",
        "                if label_json[\"arguments\"][k] == pred_json[\"arguments\"][k]:\n",
        "                    match_cnt += 1\n",
        "\n",
        "            results[\"params_value_accuracy\"][\"correct\"] += (\n",
        "                match_cnt / len(common_params)\n",
        "            )\n",
        "            results[\"params_value_accuracy\"][\"total\"] += 1\n",
        "\n",
        "    # ---------- 최종 BFCL-style 지표 ----------\n",
        "    final_results = {\n",
        "        \"tool_accuracy\": (\n",
        "            results[\"tool_selection\"][\"correct\"] /\n",
        "            results[\"tool_selection\"][\"total\"]\n",
        "            if results[\"tool_selection\"][\"total\"] > 0 else 0.0\n",
        "        ),\n",
        "        \"param_key_accuracy\": (\n",
        "            results[\"params_selection\"][\"correct\"] /\n",
        "            results[\"params_selection\"][\"total\"]\n",
        "            if results[\"params_selection\"][\"total\"] > 0 else 0.0\n",
        "        ),\n",
        "        \"param_value_accuracy\": (\n",
        "            results[\"params_value_accuracy\"][\"correct\"] /\n",
        "            results[\"params_value_accuracy\"][\"total\"]\n",
        "            if results[\"params_value_accuracy\"][\"total\"] > 0 else 0.0\n",
        "        ),\n",
        "        \"hallucination_rate\": (\n",
        "            results[\"hallucinated_args\"][\"count\"] / tool_call_count\n",
        "            if tool_call_count > 0 else 0.0\n",
        "        ),\n",
        "        \"over_call_rate\": (\n",
        "            results[\"over_call\"][\"count\"] / len(labels)\n",
        "        ),\n",
        "        \"under_call_rate\": (\n",
        "            results[\"under_call\"][\"count\"] / len(labels)\n",
        "        ),\n",
        "        \"format_error_rate\": (\n",
        "            results[\"format_error\"][\"count\"] / tool_call_count\n",
        "            if tool_call_count > 0 else 0.0\n",
        "        ),\n",
        "        \"total_tool_calls\": tool_call_count\n",
        "    }\n",
        "\n",
        "    return final_results\n"
      ],
      "metadata": {
        "id": "HF3EaDfX-s5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = df[\"label\"].tolist()\n",
        "preds = df[\"output\"].tolist()\n",
        "\n",
        "results = evaluate_function_calls_bfcl(labels, preds)\n",
        "\n",
        "for metric, value in results.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"{metric}: {value:.2%}\")\n",
        "    else:\n",
        "        print(f\"{metric}: {value}\")\n"
      ],
      "metadata": {
        "id": "0czrY_UW_S9O"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}